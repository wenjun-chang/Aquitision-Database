{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing Company 10Ks From the SEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scraping packages\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import os\n",
    "import unicodedata\n",
    "from time import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for getting SEC files as well as CIK numbers\n",
    "# FilingType gives the 100 type of files to choose from\n",
    "# Filing is the class of fetched company items\n",
    "from secedgar.filings import Filing, FilingType\n",
    "\n",
    "# CIKLookup is for looking at cik of companies, with either name or ticker, case-insensitive\n",
    "from secedgar.filings.cik_lookup import CIKLookup\n",
    "# get dict for all ticker/name -> CIK mapping\n",
    "from secedgar.utils import get_cik_map\n",
    "\n",
    "# datetime is needed for start_date or end_date in Filing()\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\happy\\OneDrive - California Institute of Technology\\Desktop\\Blackrock-Caltech Research\\Aquitision-Database\\code\\industryRank_nonTop_acq.csv\n",
      "['AECOM Technology Corp', 'Airgas Inc', 'Allergan Inc', 'Allergan PLC', 'Bed Bath & Beyond Inc', 'BioMarin Pharmaceutical Inc', 'Bottomline Technologies Inc', 'CAVU Resources Inc', 'Carver Corp', 'Comcast Corp', 'DaVita HealthCare Partners Inc', 'DaVita Inc', 'Delek Us Holdings Inc', 'Dun & Bradstreet Corp', 'Emerson Electric Co', 'Federal Signal Corp', 'Ferro Corp', 'Hanger Inc', 'Hanger Orthopedic Group Inc', 'Harman Intl Industries Inc', 'Hercules Offshore Inc', 'Latteno Food Corp', 'Measurement Specialties Inc', 'Microsoft Corp', 'Mistras Group Inc', 'Monotype Imaging Holdings Inc', 'NCI Inc', 'NuStar Energy LP', 'Omnicare Inc', 'Papa Bello Enterprises Inc', 'Regency Energy Partners LP', 'ResMed Inc', 'Superior Well Services Inc', 'Time Warner Cable Inc', 'VCA Antech Inc', 'VCA Inc', 'Vanguard Natural Resources LLC', 'Vonage Holdings Corp', 'WESCO International Inc', 'Zayo Group Holdings Inc']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\secedgar\\filings\\cik_validator.py:113: UserWarning: Lookup 'Airgas Inc' will be skipped.\n",
      "                          Found multiple companies matching 'Airgas Inc':\n",
      "                          AIRGAS INCSIC: 5084 - WHOLESALE-INDUSTRIAL MACHINERY & EQUIPMENT\n",
      "AIRGAS INC EMPLOYEE BENEFITS TRUST\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  CAVU Resources Inc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\secedgar\\filings\\cik_validator.py:113: UserWarning: Lookup 'Comcast Corp' will be skipped.\n",
      "                          Found multiple companies matching 'Comcast Corp':\n",
      "                          COMCAST HOLDINGS CORPSIC: 4841 - CABLE & OTHER PAY TELEVISION SERVICES\n",
      "COMCAST CORPSIC: 4841 - CABLE & OTHER PAY TELEVISION SERVICES\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  Delek Us Holdings Inc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\secedgar\\filings\\cik_validator.py:113: UserWarning: Lookup 'Dun & Bradstreet Corp' will be skipped.\n",
      "                          Found multiple companies matching 'Dun & Bradstreet Corp':\n",
      "                          DEX ONE CorpSIC: 7310 - SERVICES-ADVERTISING\n",
      "MOODYS CORP /DE/SIC: 7320 - SERVICES-CONSUMER CREDIT REPORTING, COLLECTION AGENCIES\n",
      "DUN & BRADSTREET CORP/NWSIC: 7320 - SERVICES-CONSUMER CREDIT REPORTING, COLLECTION AGENCIES\n",
      "  warnings.warn(warning_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  Hanger Inc\n",
      "error:  Harman Intl Industries Inc\n",
      "error:  Hercules Offshore Inc\n",
      "error:  Mistras Group Inc\n",
      "error:  NCI Inc\n",
      "error:  NuStar Energy LP\n",
      "error:  Superior Well Services Inc\n",
      "error:  Vanguard Natural Resources LLC\n",
      "error:  Zayo Group Holdings Inc\n"
     ]
    }
   ],
   "source": [
    "# get filepath \n",
    "relative_fp = os.path.realpath(os.getcwd())\n",
    "fp = relative_fp + '\\\\industryRank_nonTop_acq.csv'\n",
    "print(fp)\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(fp)\n",
    "#df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "tickers = df.loc[df['sic1']!=6, 'AcquirorName']#df['AcquirorName']\n",
    "tickers = list(np.unique(tickers))\n",
    "print(tickers)\n",
    "# get CIK number for each firm\n",
    "CIK_dict = {}\n",
    "error_tickers = []\n",
    "for ticker in tickers: \n",
    "    try: \n",
    "        CIK_dict.update(CIKLookup(ticker).lookup_dict)\n",
    "    except: \n",
    "        print('error: ', ticker)\n",
    "        error_tickers.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26  CIK Available\n",
      "40  Tickers Given\n",
      "['Airgas Inc', 'CAVU Resources Inc', 'Comcast Corp', 'Delek Us Holdings Inc', 'Dun & Bradstreet Corp', 'Hanger Inc', 'Harman Intl Industries Inc', 'Hercules Offshore Inc', 'Mistras Group Inc', 'NCI Inc', 'NuStar Energy LP', 'Superior Well Services Inc', 'Vanguard Natural Resources LLC', 'Zayo Group Holdings Inc']\n"
     ]
    }
   ],
   "source": [
    "print(len(CIK_dict), ' CIK Available')\n",
    "print(len(tickers), ' Tickers Given')\n",
    "#print(error_tickers)\n",
    "print([i for i in tickers if i not in list(CIK_dict.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AECOM Technology Corp': '0000868857',\n",
       " 'Allergan Inc': '0000850693',\n",
       " 'Allergan PLC': '0001578845',\n",
       " 'Bed Bath & Beyond Inc': '0000886158',\n",
       " 'BioMarin Pharmaceutical Inc': '0001048477',\n",
       " 'Bottomline Technologies Inc': '0001073349',\n",
       " 'Carver Corp': '0000766177',\n",
       " 'DaVita HealthCare Partners Inc': '0000927066',\n",
       " 'DaVita Inc': '0000927066',\n",
       " 'Emerson Electric Co': '0000032604',\n",
       " 'Federal Signal Corp': '0000277509',\n",
       " 'Ferro Corp': '0000035214',\n",
       " 'Hanger Orthopedic Group Inc': '0000722723',\n",
       " 'Latteno Food Corp': '0000743241',\n",
       " 'Measurement Specialties Inc': '0000778734',\n",
       " 'Microsoft Corp': '0000789019',\n",
       " 'Monotype Imaging Holdings Inc': '0001385292',\n",
       " 'Omnicare Inc': '0000353230',\n",
       " 'Papa Bello Enterprises Inc': '0001461145',\n",
       " 'Regency Energy Partners LP': '0001338613',\n",
       " 'ResMed Inc': '0000943819',\n",
       " 'Time Warner Cable Inc': '0001377013',\n",
       " 'VCA Antech Inc': '0000817366',\n",
       " 'VCA Inc': '0000817366',\n",
       " 'Vonage Holdings Corp': '0001272830',\n",
       " 'WESCO International Inc': '0000929008',\n",
       " 'Airgas Inc': '0000804212',\n",
       " 'CAVU Resources Inc': '0001461679',\n",
       " 'Comcast Corp': '0001166691',\n",
       " 'Delek Us Holdings Inc': '0001694426',\n",
       " 'Dun & Bradstreet Corp': '0001115222',\n",
       " 'Hanger Inc': '0000722723',\n",
       " 'Harman Intl Industries Inc': '0000800459',\n",
       " 'Hercules Offshore Inc': '0001330849',\n",
       " 'Mistras Group Inc': '0001436126',\n",
       " 'NCI Inc': '0001334478',\n",
       " 'NuStar Energy LP': '0001110805',\n",
       " 'Superior Well Services Inc': '0001323715',\n",
       " 'Zayo Group Holdings Inc': '0001608249'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from secedgar.utils import get_cik_map\n",
    "\n",
    "a= get_cik_map(key=\"title\")\n",
    "for ticker in tickers:\n",
    "    if ticker.upper() not in a.keys():\n",
    "        print(ticker)\n",
    "'''\n",
    "missing = {\n",
    "    'Airgas Inc': '0000804212', \n",
    "    'CAVU Resources Inc': '0001461679', \n",
    "    'Comcast Corp': '0001166691', \n",
    "    'Delek Us Holdings Inc': '0001694426',\n",
    "    'Dun & Bradstreet Corp': '0001115222',\n",
    "    'Hanger Inc': '0000722723', \n",
    "    'Harman Intl Industries Inc': '0000800459', \n",
    "    'Hercules Offshore Inc': '0001330849', \n",
    "    'Mistras Group Inc': '0001436126', \n",
    "    'NCI Inc': '0001334478', \n",
    "    'NuStar Energy LP': '0001110805', \n",
    "    'Superior Well Services Inc': '0001323715', \n",
    "    'Zayo Group Holdings Inc': '0001608249'\n",
    "}\n",
    "#'American Campus Comm Inc': '0001283630', this is bank/insurance/real estate\n",
    "\n",
    "fucked = ['Vanguard Natural Resources LLC']\n",
    "CIK_dict.update(missing)\n",
    "CIK_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the info for each firm in df, before we start web scraping 10Ks\n",
    "firms_info = []\n",
    "\n",
    "for company_ticker in tickers:\n",
    "    if company_ticker not in CIK_dict.keys():\n",
    "        continue\n",
    "    company_cik = CIK_dict[company_ticker]\n",
    "    \n",
    "    # base url for every search 10-K page on SEC.org\n",
    "    search_10K_page_base_url = r'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=|&type=10-k'\n",
    "\n",
    "    # get firm-specific url\n",
    "    search_10K_page_url = search_10K_page_base_url.replace('|', company_cik)\n",
    "\n",
    "    # connect to page and search\n",
    "    search_page = requests.get(search_10K_page_url).content\n",
    "    soup = BeautifulSoup(search_page, 'html')\n",
    "\n",
    "    # store report numbers for each firm\n",
    "    ###can do later\n",
    "\n",
    "    # find the section denoting report number\n",
    "    for td_instance in soup.find_all('td',{'class':'small'}):\n",
    "        text = td_instance.text\n",
    "        \n",
    "        # skip 10-K/A files which are amendments\n",
    "        if '[Amend]' in text:\n",
    "            #print('10K/A file encountered')\n",
    "            continue\n",
    "        \n",
    "        # parse to get each report number\n",
    "        index_of_start = text.find('Acc-no:')\n",
    "        index_of_end = text.find('(34 Act)') # stop before '('\n",
    "\n",
    "        # some old ones has no (34 Act)\n",
    "        if index_of_end == -1:\n",
    "            index_of_end = text.find('Size:')\n",
    "\n",
    "        if index_of_start == -1 or index_of_end == -1:\n",
    "            # guess no annual report in here\n",
    "            print('no annual report in this <td> class')\n",
    "            print(td_instance.text)\n",
    "            continue\n",
    "\n",
    "        index_of_start += len('Acc-no:') # start after this string\n",
    "\n",
    "        # get report number\n",
    "        report_number = text[index_of_start: index_of_end]\n",
    "        # remove - and whitespace\n",
    "        report_number = report_number.strip().replace('-', '')\n",
    "        # find date\n",
    "        report_date = td_instance.findNext('td').text\n",
    "        # update df\n",
    "        firms_info.append([company_ticker, company_cik, report_number, report_date])\n",
    "\n",
    "#convert to dataframe\n",
    "firms_info = pd.DataFrame(firms_info, columns=['Ticker', 'CIK', '10-K Report Number', '10-K Report Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>CIK</th>\n",
       "      <th>10-K Report Number</th>\n",
       "      <th>10-K Report Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>AECOM Technology Corp</td>\n",
       "      <td>0000868857</td>\n",
       "      <td>000110465919063367</td>\n",
       "      <td>2019-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>AECOM Technology Corp</td>\n",
       "      <td>0000868857</td>\n",
       "      <td>000104746918007225</td>\n",
       "      <td>2018-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>AECOM Technology Corp</td>\n",
       "      <td>0000868857</td>\n",
       "      <td>000104746917007051</td>\n",
       "      <td>2017-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>AECOM Technology Corp</td>\n",
       "      <td>0000868857</td>\n",
       "      <td>000104746916016739</td>\n",
       "      <td>2016-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>AECOM Technology Corp</td>\n",
       "      <td>0000868857</td>\n",
       "      <td>000104746915008965</td>\n",
       "      <td>2015-11-25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Ticker         CIK  10-K Report Number 10-K Report Date\n",
       "0  AECOM Technology Corp  0000868857  000110465919063367       2019-11-13\n",
       "1  AECOM Technology Corp  0000868857  000104746918007225       2018-11-13\n",
       "2  AECOM Technology Corp  0000868857  000104746917007051       2017-11-14\n",
       "3  AECOM Technology Corp  0000868857  000104746916016739       2016-11-16\n",
       "4  AECOM Technology Corp  0000868857  000104746915008965       2015-11-25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firms_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(636, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firms_info.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/0000892482/0000892482-20-000007-index.htm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_10_K_page_url\n",
    "def get_10_K_page_url(xml_summary):\n",
    "    page_10K_url = xml_summary.replace('/FilingSummary.xml', '')\n",
    "    page_10K_url = page_10K_url[:-8] + '-' + page_10K_url[-8:]\n",
    "    page_10K_url = page_10K_url[:-6] + '-' + page_10K_url[-6:]\n",
    "    page_10K_url += '-index.htm'\n",
    "    return page_10K_url\n",
    "\n",
    "#get 10 K html text file\n",
    "def get_10_K_txt(xml_summary):\n",
    "    page_10K_url = get_10_K_page_url(xml_summary)\n",
    "    return page_10K_url.replace('-index.htm', '.txt')\n",
    "get_10_K_page_url('https://www.sec.gov/Archives/edgar/data/0000892482/000089248220000007/FilingSummary.xml')\n",
    "#get_10_K_txt('https://www.sec.gov/Archives/edgar/data/0000892482/000089248220000007/FilingSummary.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode weird strings...not really working\n",
    "def restore_windows_1252_characters(restore_string):\n",
    "    \"\"\"\n",
    "        Replace C1 control characters in the Unicode string s by the\n",
    "        characters at the corresponding code points in Windows-1252,\n",
    "        where possible.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_windows_1252(match):\n",
    "        try:\n",
    "            return bytes([ord(match.group(0))]).decode('windows-1252')\n",
    "        except UnicodeDecodeError:\n",
    "            # No character at the corresponding code point: remove it.\n",
    "            return ''\n",
    "        \n",
    "    return re.sub(r'[\\u0080-\\u0099]', to_windows_1252, restore_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# helper functions for reports without .xlm summary\n",
    "# older ones before 2012\n",
    "############################################\n",
    "def parse_acquisition_table(table_instance): # a bs4 table instance\n",
    "    acquisition_related = False\n",
    "    \n",
    "    # define a dictionary that will store the different parts of the statement.\n",
    "    table_data = {}\n",
    "    table_data['headers'] = []\n",
    "    table_data['sections'] = []\n",
    "    table_data['data'] = []\n",
    "\n",
    "    # find all the rows, figure out what type of row it is, parse the elements, and store in the statement file list.\n",
    "    for index, row in enumerate(table_instance.find_all('tr')):\n",
    "        \n",
    "        # first let's get all the elements.\n",
    "        cols = row.find_all('td')\n",
    "        \n",
    "        # get data by row\n",
    "        reg_row = [ele.text.strip() for ele in cols] # remove unicode char\n",
    "        table_data['data'].append(reg_row)\n",
    "        \n",
    "        keywords = ['acquisitions', 'business combinations'] #!!!!!!\n",
    "        if True in [keyword in element.lower() for element in reg_row for keyword in keywords]:\n",
    "            acquisition_related = True\n",
    "        '''\n",
    "        # if it's a regular row and not a section or a table header\n",
    "        if (len(row.find_all('th')) == 0 and len(row.find_all('strong')) == 0): \n",
    "            reg_row = [ele.text.strip() for ele in cols]\n",
    "            table_data['data'].append(reg_row)\n",
    "                        \n",
    "            if True in ['acquisition' in element.lower() for element in reg_row]:\n",
    "                acquisition_related = True\n",
    "            \n",
    "        # if it's a regular row and a section but not a table header\n",
    "        elif (len(row.find_all('th')) == 0 and len(row.find_all('strong')) != 0):\n",
    "            sec_row = cols[0].text.strip()\n",
    "            table_data['sections'].append(sec_row)\n",
    "            \n",
    "            if True in ['acquisition' in element.lower() for element in sec_row]:\n",
    "                acquisition_related = True\n",
    "            \n",
    "        # finally if it's not any of those it must be a header\n",
    "        elif (len(row.find_all('th')) != 0):            \n",
    "            hed_row = [unicodedata.normalize('NFKD',ele.text.strip()) for ele in row.find_all('th')]\n",
    "            table_data['headers'].append(hed_row)\n",
    "             \n",
    "            if True in ['acquisition' in element.lower() for element in hed_row]:\n",
    "                acquisition_related = True\n",
    "            \n",
    "        else:            \n",
    "            print('We encountered an error.')\n",
    "        ''' \n",
    "    '''\n",
    "    # turn to dataframe\n",
    "    table = pd.DataFrame(table_data['data'])\n",
    "    if acquisition_related: \n",
    "        # only return when it is acquisition related\n",
    "        return table\n",
    "    return # return None if not\n",
    "    '''\n",
    "    return acquisition_related\n",
    "'''\n",
    "# get acquisition related reports for older 10-Ks using .txt html url, return a list of urls where we save them\n",
    "def get_info_from_txt(txt_url, name):\n",
    "    content = requests.get(txt_url).content\n",
    "    soup = BeautifulSoup(content, 'html')\n",
    "    \n",
    "    # get notes sections for acquisition\n",
    "    stuff = []\n",
    "    bolds = soup.find_all('b')\n",
    "    if bolds is not None:\n",
    "        for bold_instance in bolds:\n",
    "            #bold_font = bold_instance.findChild('font' , recursive=False)\n",
    "            if bold_instance is not None:\n",
    "                note_html = str(bold_instance)\n",
    "                bold_text = bold_instance.text\n",
    "                \n",
    "                keywords = ['acquisitions', 'business combinations'] #!!!!!!\n",
    "                if True in [keyword in bold_text.lower() for keyword in keywords]:\n",
    "                    #first check if it is actually a title\n",
    "                    # 60 is a like loose upper bound for a note title pretty sure\n",
    "                    if len(' '.join(bold_text.split())) > 60: \n",
    "                        continue                    \n",
    "                    \n",
    "                    # just parse next 4 pages the most stupid way \n",
    "                    # but it ensures I can get everything\n",
    "                    pages = 1\n",
    "                    print(bold_text)\n",
    "                    thematic_break = bold_instance.find_next('hr',{'width':'100%'}) # page break\n",
    "                    if thematic_break is None: \n",
    "                        #try again by just finding hr\n",
    "                        thematic_break = bold_instance.find_next('hr')\n",
    "                    if thematic_break is None: \n",
    "                        # reached end of 10K\n",
    "                        continue\n",
    "                    current_instance = bold_instance.find_next()\n",
    "                    while pages <= 4 and current_instance is not None:\n",
    "                        note_html += str(current_instance)\n",
    "                        if current_instance == thematic_break: \n",
    "                            pages += 1    \n",
    "                        current_instance = current_instance.find_next()\n",
    "                    note_html += str(thematic_break)\n",
    "                    stuff.append(note_html)\n",
    "                    #############################\n",
    "                    # this is the next note section's title\n",
    "                    next_bold_instance = bold_instance.find_next('b')\n",
    "                    instance = next_bold_instance\n",
    "                    while instance != next_bold_instance:\n",
    "                        # new way\n",
    "                        note_html += str(instance)\n",
    "                        instance = instance.find_next()\n",
    "                    div_to_stop = next_bold_instance.parent #maybe find parent div\n",
    "                    # the paragraphs we want\n",
    "                    div_instance = bold_instance.find_next('div')\n",
    "                    while div_instance != div_to_stop:\n",
    "                        #paragraph = unicodedata.normalize('NFKD', div_instance.text)\n",
    "                        #acquisition_notes += paragraph\n",
    "                        # new way\n",
    "                        note_html += str(div_instance)\n",
    "                        div_instance = div_instance.find_next('div')\n",
    "                    #stuff.append(acquisition_notes.encode('ascii', 'ignore')) # remove unicode char\n",
    "                    # new way\n",
    "                    ##############################   \n",
    "    \n",
    "    # get table sections for acquisition\n",
    "    tables = soup.find_all('table')\n",
    "    if tables is not None:\n",
    "        for table_instance in tables:\n",
    "                acquisition_related = parse_acquisition_table(table_instance)\n",
    "                if acquisition_related: # if table is not None:\n",
    "                    stuff.append(table_instance)\n",
    "    \n",
    "    report_urls = []\n",
    "    # for all related stuff, write to html and get the link for them\n",
    "    if stuff:\n",
    "        for index, html_instance in enumerate(stuff):\n",
    "            report_url = add_link_and_write(html_instance, name + '_report_' + str(index), dir_name='sec_data')\n",
    "            report_urls.append(['report ' + str(index), report_url])\n",
    "    return report_urls\n",
    "'''\n",
    "# add a parent tag for the html instanse parsed\n",
    "def wrap(to_wrap, wrap_in):\n",
    "    contents = to_wrap.replace_with(wrap_in)\n",
    "    wrap_in.append(contents)\n",
    "    \n",
    "# add link to the html instance we have s.t. we can write it to a html file and call later\n",
    "def add_link_and_write(html_instance, link_name, dir_name='sec_data'):\n",
    "    # turn tag into a bs4 instance\n",
    "    #info = BeautifulSoup(str(html_instance))\n",
    "    \n",
    "    # why I need this????cannot recall\n",
    "    #wrap(info.table, info.new_tag('a'))\n",
    "    #url = r\"file:///C:\\Users\\happy\\OneDrive - California Institute of Technology\\Desktop\\{}\\{}.html\".format(dir_name, link_name)\n",
    "    #info.a['href'] = url\n",
    "    #<a href=\"where/you/want/the/link/to/go\">text of the link</a>\n",
    "    \n",
    "    # write to html\n",
    "    dir_name = dir_name.replace(' ', '_')\n",
    "    link_name = link_name.replace(' ', '_')\n",
    "    f = open(os.path.realpath(os.getcwd())+'\\\\{}\\\\{}.html'.format(dir_name, link_name),'wb')\n",
    "    f.write(html_instance.encode('ascii', 'ignore'))\n",
    "    #f.write(str(info).encode('ascii', 'ignore'))\n",
    "    f.close()\n",
    "    return r'http://makeresearchgreatagain.com/sec_files/' + link_name + '.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this becomes a helper function\n",
    "def get_info_from_txt_single_instance(txt_url, year, name='', max_word=50, prev_year_tolerance=10, max_pages=5, instance_type='b'):\n",
    "    content = requests.get(txt_url).content\n",
    "    soup = BeautifulSoup(content, 'html')\n",
    "    \n",
    "    # get notes sections for acquisition\n",
    "    stuff = []\n",
    "    bolds = soup.find_all(instance_type)\n",
    "    \n",
    "    # special case designed for microsoft since its section title is underlined, not bold\n",
    "    if '/0000789019/' in txt_url: # 0000789019 is MSFT CIK\n",
    "        bolds = soup.find_all('u')\n",
    "        \n",
    "    if bolds is not None:\n",
    "        #change to while loop to skip the bold instances already seen in previous parsing\n",
    "        i = 0 # index\n",
    "        while i < len(bolds):        \n",
    "            bold_instance = bolds[i]\n",
    "            #bold_font = bold_instance.findChild('font' , recursive=False)\n",
    "            if bold_instance is not None:\n",
    "                bold_text = bold_instance.text\n",
    "                \n",
    "                note_titles = ['acquisition', 'business combination'] #!!!!!\n",
    "                # special case\n",
    "                if '/0001377013/' in txt_url: # 0000789019 is Time Warner Cable CIK\n",
    "                    note_titles = ['acquisition', 'business combination', 'transaction']\n",
    "                    \n",
    "                if True in [title in bold_text.lower() for title in note_titles]:\n",
    "                    #first check if it is actually a title\n",
    "                    # 60 is a like loose upper bound for a note title pretty sure\n",
    "                    if len(' '.join(bold_text.split())) > 60:\n",
    "                        i += 1 #increment\n",
    "                        continue\n",
    "                    \n",
    "                    #note_html = ''\n",
    "                    \n",
    "                    #case 1: note N is present in title: \n",
    "                    # easy...keep going until hit the bold instance with note N + 1\n",
    "                    '''\n",
    "                    not working\n",
    "                    if 'note' in bold_text.lower():\n",
    "                        print('case 1')\n",
    "                        title_words_list = bold_text.lower().split()\n",
    "                        index_of_note = title_words_list.index('note')\n",
    "                        if not title_words_list[index_of_note+1].isnumeric():\n",
    "                            raise Exception('this note section has no number!!!!!')\n",
    "                        else: \n",
    "                            next_note_header = 'note' + str(int(title_words_list[index_of_note+1]) + 1)\n",
    "                            print(next_note_header)\n",
    "                            note_html = str(bold_instance)\n",
    "                            current_instance = bold_instance.find_next()\n",
    "                            while current_instance is not None and next_note_header not in current_instance.text.lower().replace(' ', ''):\n",
    "                                note_html += str(current_instance)   \n",
    "                                current_instance = current_instance.find_next()\n",
    "                    '''\n",
    "                    #case 2: use this 5 pages info as worst case html\n",
    "                    # just parse next 5 pages the most stupid way \n",
    "                    # but it ensures I can get everything\n",
    "                    #else: \n",
    "                    pages = 1\n",
    "                    note_html = str(bold_instance)\n",
    "                    thematic_break = bold_instance.find_next('hr',{'width':'100%'}) # page break\n",
    "                    if thematic_break is None: \n",
    "                        #try again by just finding hr\n",
    "                        thematic_break = bold_instance.find_next('hr')\n",
    "                    if thematic_break is None: \n",
    "                        # reached end of 10K\n",
    "                        i += 1 #increment\n",
    "                        continue\n",
    "                    current_instance = bold_instance.find_next()\n",
    "                    while pages <= max_pages and current_instance is not None:\n",
    "                        note_html += str(current_instance)\n",
    "                        #print(current_instance.name)\n",
    "                        if current_instance == thematic_break:#'<hr' in str(current_instance): \n",
    "                            pages += 1\n",
    "                        current_instance = current_instance.find_next()\n",
    "                    note_html += str(thematic_break)\n",
    "                    \n",
    "                    print('finished get html')\n",
    "                    \n",
    "                    #now we have the html we want to refine on\n",
    "                    #get its text\n",
    "                    #return note_html\n",
    "                    note_text = BeautifulSoup(note_html.encode('ascii', 'ignore').decode().replace('\\n', ' ')).text\n",
    "                    note_text = note_text.split()\n",
    "                    \n",
    "                    #after passing max_char wihout seeing a keyword, we stop and get length of the strings so far\n",
    "                    prev_year_tolerance = prev_year_tolerance #give a tolerance for seeing year other than X and X-1\n",
    "                    prev_year_encountered = 0\n",
    "                    distance = 0\n",
    "                    curr_index = 0\n",
    "                    #also need to count for $ since we might encounter a table that is relevant\n",
    "                    keywords = ['acquisition', 'acquire', 'merger', 'business combination', 'asset', 'million', \n",
    "                                'billion', 'thousand', 'fair', 'value', 'purchase', 'price', 'paid', '$', \n",
    "                                'agreement', 'transaction', 'ownership', 'bought', 'cost'] #, 'goodwill'\n",
    "                    previous_19_years = np.arange(int(year)-20, int(year)-1).astype(str) #previous fiscal years except last fiscal year in string\n",
    "                    \n",
    "                    while distance < max_word:\n",
    "                        if curr_index >= len(note_text) or prev_year_encountered > prev_year_tolerance: \n",
    "                            break\n",
    "                        if True in [prev_year in note_text[curr_index] for prev_year in previous_19_years]:\n",
    "                            prev_year_encountered += 1 # do not want this year's report to report acquisitions in previous years\n",
    "                        if True in [keyword in note_text[curr_index].lower() for keyword in keywords]:\n",
    "                            distance = 0\n",
    "                        else: \n",
    "                            distance += 1 #len(note_text[curr_index])\n",
    "                        curr_index += 1\n",
    "                    \n",
    "                    refined_text = note_text[:curr_index-1]\n",
    "                    \n",
    "                    print('finished get text')\n",
    "                    '''\n",
    "                    #compute information ratio = length(refined_text) / length(note_text)\n",
    "                    info_ratio = len(' '.join(refined_text)) / len(' '.join(note_text))\n",
    "                    \n",
    "                    #find the length we want for html instance\n",
    "                    desired_length = round(len(note_html) * info_ratio)\n",
    "                    \n",
    "                    refined_html = note_html[:desired_length]\n",
    "                    refined_html = refined_html[:(refined_html.rfind('>')+1)]\n",
    "                    \n",
    "                    print(refined_html[-100:])\n",
    "                    '''\n",
    "                    desired_length = len(''.join(refined_text))\n",
    "                    refined_html = str(bold_instance)\n",
    "                    current_instance = bold_instance\n",
    "                    prev_instance = ''\n",
    "                    dummy_html = str(bold_instance)\n",
    "                    #used another way: re to parse only text\n",
    "                    while len(re.sub(\"\\<.*?\\>\", \"\", refined_html).replace(' ', '').replace('\\n', '')) < desired_length: \n",
    "                        if len(dummy_html) > len(note_html) or current_instance is None:\n",
    "                            break\n",
    "                        prev_instance = current_instance\n",
    "                        current_instance = current_instance.find_next()\n",
    "                        \n",
    "                        #checks if another bold instance appeared\n",
    "                        if '<b>' in str(current_instance) and True in [title in current_instance.text.lower() for title in note_titles]:\n",
    "                            i += 1 #skip next instance\n",
    "                        \n",
    "                        while re.sub(\"\\<.*?\\>\", \"\", str(prev_instance)) == re.sub(\"\\<.*?\\>\", \"\", str(current_instance)) or prev_instance.text.lower() == current_instance.text.lower():\n",
    "                            prev_instance = current_instance\n",
    "                            current_instance = current_instance.find_next()\n",
    "                            dummy_html += str(current_instance)\n",
    "                            if '<b>' in str(current_instance) and True in [title in current_instance.text.lower() for title in note_titles]:\n",
    "                                i += 1 #skip next instance\n",
    "                                \n",
    "                        if str(prev_instance) == str(current_instance):\n",
    "                            print('previous and current instance are the same')\n",
    "                            \n",
    "                        refined_html += str(current_instance)\n",
    "                        dummy_html += str(current_instance)\n",
    "                    \n",
    "                    print('finished refine html')\n",
    "            \n",
    "                    #update\n",
    "                    stuff.append(refined_html)\n",
    "            i += 1 #increment i\n",
    "    '''\n",
    "    skip cuz we want to write aggregately to one file per note\n",
    "    report_urls = []\n",
    "    # for all related stuff, write to html and get the link for them\n",
    "    if stuff:\n",
    "        for index, html_instance in enumerate(stuff):\n",
    "            report_url = add_link_and_write(html_instance, name + '_report_' + str(index), dir_name='sec_data')\n",
    "            report_urls.append(['report ' + str(index), report_url])\n",
    "    return report_urls\n",
    "    '''\n",
    "    return stuff, len(stuff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acquisition related reports for older 10-Ks using .txt html url, return a list of urls where we save them\n",
    "def get_info_from_txt(txt_url, year, name='', max_word=50, prev_year_tolerance=10, max_pages=5):\n",
    "    html = ''\n",
    "    N = 0\n",
    "    instances = ['b', 'h1', 'strong', 'div'] #'p' and 'td, 'u'should be checked but too common\n",
    "    for instance in instances:\n",
    "        print('instance:', instance)\n",
    "        curr_html, curr_N = get_info_from_txt_single_instance(txt_url, year, name, max_word, prev_year_tolerance, \n",
    "                                                      max_pages, instance_type=instance)\n",
    "        if N > 0:\n",
    "            html = curr_html # a list\n",
    "            N = curr_N\n",
    "            return html, N\n",
    "    return html, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instance:  b\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n"
     ]
    }
   ],
   "source": [
    "link =r'https://www.sec.gov/Archives/edgar/data/0001166691/0001193125-09-033975.txt'\n",
    "link = r'https://www.sec.gov/Archives/edgar/data/0000789019/0001193125-11-200680.txt'\n",
    "#instance = get_info_from_txt(link, '11111', '2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the end date of the fiscal year \n",
    "def get_fiscal_year(page_10_K_url):\n",
    "    content = requests.get(page_10_K_url).content\n",
    "    soup = BeautifulSoup(content, 'html')\n",
    "    \n",
    "    # get head titles in 10K page sections for acquisition\n",
    "    head_titles = soup.find_all('div', {'class':'infoHead'})\n",
    "    \n",
    "    date_ended = None\n",
    "    for header in head_titles:\n",
    "        if header.text.lower() == 'period of report':\n",
    "            date_ended = header.find_next('div', {'class':'info'}).text\n",
    "    return date_ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2015-12-31'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xxxxx = 'https://www.sec.gov/Archives/edgar/data/0001115222/0001115222-16-000045-index.htm'\n",
    "get_fiscal_year(xxxxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(txt_url, name=''):\n",
    "    content = requests.get(txt_url).content\n",
    "    soup = BeautifulSoup(content, 'html')\n",
    "    \n",
    "    # get notes sections for acquisition\n",
    "    tables = soup.find_all('table')\n",
    "    print(len(tables))\n",
    "    \n",
    "    tables_url = []\n",
    "    if tables is not None:\n",
    "        for i in range(len(tables)):\n",
    "            tables_url.append(add_link_and_write(str(tables[i]), name + '_' + str(i), dir_name='table'))\n",
    "    return tables_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497\n"
     ]
    }
   ],
   "source": [
    "link =r'https://www.sec.gov/Archives/edgar/data/0001166691/0001193125-09-033975.txt'\n",
    "get_table = get_table(link, 'table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[document]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BeautifulSoup(str('<hr align=\"CENTER\" noshade=\"\" size=\"4\" width=\"100%\"/>')).name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "link =r'https://www.sec.gov/Archives/edgar/data/0001166691/0001193125-09-033975.txt'\n",
    "#instance = get_info_from_txt(link, '11111', '2009')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'<hr align=\"CENTER\" noshade=\"\" size=\"4\" width=\"100%\"/>'.index('<hr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. used info ratio to estimate the length of html to get\\n2. upper limit is 4 pages\\n3. immediately stop adding html if fiscal year 20XX earlier than last fiscal year shows up\\n4. for .xlm omits tables with name detail or table on it...only want general note\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. used info ratio to estimate the length of html to get\n",
    "2. upper limit is 4 pages\n",
    "3. immediately stop adding html if fiscal year 20XX earlier than last fiscal year shows up\n",
    "4. for .xlm omits tables with name detail or table on it...only want general note\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(str(instance).lower().index('</'))\n",
    "#print(str(instance).lower().index('acquisitions'))\n",
    "#len(str(instance)[6:184])\n",
    "#len('note 2 . business combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def find_relevent_reports(firms_info, start_year=2007, end_year=2017, max_word=50, \n",
    "                          prev_year_tolerance=10, max_pages=5):\n",
    "    # to be returned\n",
    "    relevant_table_links = []\n",
    "    # to be returned and saved for Professor\n",
    "    #relevant_table_links = pd.DataFrame(' ', index=list(firms_info['Ticker'].unique()), columns=np.arange(1950,2021)[::-1].astype(str))\n",
    "    \n",
    "    # failed to parse one\n",
    "    failures = 0\n",
    "    failed_table_links = []\n",
    "    \n",
    "    # base link for every xlm doc for each 10-K\n",
    "    xlm_base_url = r'https://www.sec.gov/Archives/edgar/data/CIK/report_number/FilingSummary.xml'\n",
    "    \n",
    "    # for each 10-K we have\n",
    "    for i in range(firms_info.shape[0]):\n",
    "        \n",
    "        #if i < 30 or i > 40:# or i > 200:\n",
    "        #    continue\n",
    "        #if report_info_df['Ticker'] != 'ATVI':\n",
    "        #    continue\n",
    "        print(i)\n",
    "        \n",
    "        # current row\n",
    "        report_info_df = firms_info.iloc[i,:]\n",
    "        current_year = int(report_info_df['10-K Report Date'].split('-')[0])\n",
    "        \n",
    "        #if report_info_df['Ticker'] != 'Dun & Bradstreet Corp':# or current_year != 2010:\n",
    "        #    continue\n",
    "\n",
    "        if current_year > int(end_year) or current_year < int(start_year):\n",
    "            continue\n",
    "            \n",
    "        print(report_info_df['Ticker'])\n",
    "        print(report_info_df['10-K Report Date'])\n",
    "        print('-'*100)\n",
    "        \n",
    "        # xlm link for current 10-K\n",
    "        xml_summary = xlm_base_url.replace('CIK', report_info_df['CIK']).replace('report_number', report_info_df['10-K Report Number'])\n",
    "    \n",
    "        # define a new base url that represents the filing folder. This will come in handy when we need to download the reports.\n",
    "        base_url = xml_summary.replace('FilingSummary.xml', '')\n",
    "        \n",
    "        print(xml_summary)\n",
    "        print(get_10_K_page_url(xml_summary))\n",
    "        print(get_10_K_txt(xml_summary))\n",
    "        \n",
    "        try:\n",
    "            # request and parse the content\n",
    "            content = requests.get(xml_summary).content\n",
    "            soup = BeautifulSoup(content, 'lxml')\n",
    "            \n",
    "        except:\n",
    "            #print(report_info_df['Company Name'])\n",
    "            print('Link does not exist')\n",
    "            print(xml_summary)\n",
    "            print('-'*100)\n",
    "\n",
    "        # find the 'myreports' tag because this contains all the individual reports submitted.\n",
    "        reports = soup.find('myreports')\n",
    "        page_10_K_url = get_10_K_page_url(xml_summary)\n",
    "        txt_url = get_10_K_txt(xml_summary)\n",
    "        end_date_fiscal_year = get_fiscal_year(page_10_K_url)\n",
    "        \n",
    "        #content to write altogether for each company-year if not .xlm\n",
    "        report_html = ''\n",
    "        #num_reports = 0\n",
    "        \n",
    "        if reports == None:\n",
    "            # this is the case when the 10-Ks are early than (including 2011)\n",
    "            # no .xlm summary exist so directly parse .txt html file\n",
    "            print('Directly parse .txt html file')\n",
    "            try:              \n",
    "                html_contents, N = get_info_from_txt(txt_url, year=current_year, max_word=max_word, \n",
    "                                                     prev_year_tolerance=prev_year_tolerance, max_pages=max_pages)\n",
    "                report_html += '<br /><hr noshade=\"\"/><hr noshade=\"\"/><br />'.join(html_contents)\n",
    "                #num_reports += N\n",
    "                \n",
    "                '''\n",
    "                reports_url = get_info_from_txt(txt_url, name=report_info_df['Ticker']+'_'+report_info_df['10-K Report Date'], \n",
    "                                                year=current_year, max_word=max_word)\n",
    "                \n",
    "                # of form [report name, report url]\n",
    "                for report_name_and_url in reports_url:\n",
    "\n",
    "                    relevant_table_links.append(report_info_df.tolist() + [report_name_and_url[1], page_10_K_url])\n",
    "                    #relevant_table_links.loc[report_info_df['Ticker'], report_info_df['10-K Report Date'].split('-')[0]] += report_name_and_url[1] + ' | '\n",
    "\n",
    "                    print(page_10_K_url)\n",
    "                    print(report_name_and_url[0])\n",
    "                    print(report_name_and_url[1])\n",
    "                    print('-'*100)\n",
    "                '''\n",
    "\n",
    "            # real issue didnt catch\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('got fucked 1!')\n",
    "                print(txt_url)\n",
    "                print('-'*100)\n",
    "                failures += 1\n",
    "                failed_table_links.append([report_info_df['Ticker'], report_info_df['10-K Report Date'], page_10_K_url, txt_url])\n",
    "                continue\n",
    "            \n",
    "        else: \n",
    "            # loop through each report in the 'myreports' tag but avoid the last one as this will cause an error.\n",
    "            try:\n",
    "                count = 0 \n",
    "                for report in reports.find_all('report')[:-1]:\n",
    "\n",
    "                    # acquisition related\n",
    "                    # what we want\n",
    "                    if 'acquisition' in report.shortname.text.lower() or 'aquisition' in report.longname.text.lower():\n",
    "                        \n",
    "                        #####we dont want tables and detailed stuff for RA, only note\n",
    "                        forms_not_desired = ['table', 'detail']\n",
    "                        if True in [form in report.shortname.text.lower() for form in forms_not_desired]:\n",
    "                            continue\n",
    "\n",
    "                        # update our list\n",
    "                        table_name = report.shortname.text # report.longname.text # both long or short is fine\n",
    "                        table_url = base_url + report.htmlfilename.text\n",
    "                        \n",
    "                        relevant_table_links.append(report_info_df.tolist() + [table_url, page_10_K_url, end_date_fiscal_year])\n",
    "                        #relevant_table_links.append(report_info_df.tolist() + [table_url, page_10_K_url, ', '.join([str(current_year), str(current_year-1)])])\n",
    "                        #relevant_table_links.loc[report_info_df['Ticker'], report_info_df['10-K Report Date'].split('-')[0]] += table_url + ' | '\n",
    "                        count += 1\n",
    "                        print(page_10_K_url)\n",
    "                        print(table_name)\n",
    "                        print(table_url)\n",
    "                        print('-'*100)\n",
    "                if count == 0:\n",
    "                    relevant_table_links.append(report_info_df.tolist() + ['No Acquisition', page_10_K_url, end_date_fiscal_year])\n",
    "                \n",
    "                report_html = 'got xlm' # for blank row printing\n",
    "                        \n",
    "            except:\n",
    "                # some .xlm summary have acquisition related report but not .htm instance\n",
    "                # need parse .html directly as well\n",
    "                print('Directly parse .txt html file')\n",
    "                try:\n",
    "                    html_contents, N = get_info_from_txt(txt_url, year=current_year, max_word=max_word, \n",
    "                                                     prev_year_tolerance=prev_year_tolerance, max_pages=max_pages)\n",
    "                    report_html += '<br /><hr noshade=\"\"/><hr noshade=\"\"/><br />'.join(html_contents)\n",
    "                    #num_reports += N\n",
    "                    \n",
    "                    '''\n",
    "                    reports_url = get_info_from_txt(txt_url, name=report_info_df['Ticker']+'_'+report_info_df['10-K Report Date'], \n",
    "                                                    year=current_year, max_word=max_word)\n",
    "\n",
    "                    # of form [report name, report url]\n",
    "                    for report_name_and_url in reports_url:\n",
    "                        \n",
    "                        relevant_table_links.append(report_info_df.tolist() + [report_name_and_url[1], page_10_K_url])\n",
    "                        #relevant_table_links.loc[report_info_df['Ticker'], report_info_df['10-K Report Date'].split('-')[0]] += report_name_and_url[1] + ' | '\n",
    "                        \n",
    "                        print(page_10_K_url)\n",
    "                        print(report_name_and_url[0])\n",
    "                        print(report_name_and_url[1])\n",
    "                        print('-'*100)\n",
    "                    '''\n",
    "\n",
    "                # real issue didnt catch\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print('got fucked 2!')\n",
    "                    print(txt_url)\n",
    "                    failures += 1\n",
    "                    failed_table_links.append([report_info_df['Ticker'], report_info_df['10-K Report Date'], page_10_K_url, txt_url])\n",
    "                    continue\n",
    "        \n",
    "        #write html content to file if not none\n",
    "        if report_html.strip() != '':\n",
    "            if report_html != 'got xlm':\n",
    "                report_url = add_link_and_write(report_html, report_info_df['Ticker']+'_'+report_info_df['10-K Report Date'] + '_report', dir_name='sec_data')\n",
    "                relevant_table_links.append(report_info_df.tolist() + [report_url, page_10_K_url, end_date_fiscal_year])\n",
    "                print(report_url)\n",
    "        else:\n",
    "            relevant_table_links.append(report_info_df.tolist() + ['No Acquisition', page_10_K_url, end_date_fiscal_year])\n",
    "                                                \n",
    "                \n",
    "                \n",
    "    relevant_table_links = pd.DataFrame(relevant_table_links, columns=['Compnay Name', 'CIK', '10-K Report Number', '10-K Report Date',\n",
    "                                                                       'Notes Link', '10-K Report Page URL', 'End Date of Fiscal Year'])\n",
    "    relevant_table_links = relevant_table_links.drop(columns=['10-K Report Number', '10-K Report Date'])\n",
    "    \n",
    "    failed_table_links = pd.DataFrame(failed_table_links, columns=['Compnay Name', '10-K Report Date', '10-K Report Page URL', '10-K Report .html URL'])\n",
    "\n",
    "    return relevant_table_links, failed_table_links, failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "AECOM Technology Corp\n",
      "2017-11-14\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746917007051/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-17-007051-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-17-007051.txt\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-17-007051-index.htm\n",
      "Business Acquisitions, Goodwill and Intangible Assets\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746917007051/R10.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3\n",
      "AECOM Technology Corp\n",
      "2016-11-16\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746916016739/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-16-016739-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-16-016739.txt\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-16-016739-index.htm\n",
      "Business Acquisitions, Goodwill and Intangible Assets\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746916016739/R10.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "4\n",
      "AECOM Technology Corp\n",
      "2015-11-25\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746915008965/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-15-008965-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-15-008965.txt\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-15-008965-index.htm\n",
      "Business Acquisitions, Goodwill and Intangible Assets\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746915008965/R11.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "5\n",
      "AECOM Technology Corp\n",
      "2014-11-17\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746914009283/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-14-009283-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-14-009283.txt\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-14-009283-index.htm\n",
      "Business Acquisitions, Goodwill and Intangible Assets\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746914009283/R11.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "6\n",
      "AECOM Technology Corp\n",
      "2013-11-13\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746913010484/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-13-010484-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-13-010484.txt\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-13-010484-index.htm\n",
      "Business Acquisitions, Goodwill, and Intangible Assets\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746913010484/R11.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "7\n",
      "AECOM Technology Corp\n",
      "2012-11-19\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746912010679/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-12-010679-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-12-010679.txt\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-12-010679-index.htm\n",
      "Business Acquisitions, Goodwill, and Intangible Assets\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746912010679/R10.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "8\n",
      "AECOM Technology Corp\n",
      "2011-11-21\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746911009697/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-11-009697-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-11-009697.txt\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-11-009697-index.htm\n",
      "Business Acquisitions, Goodwill, and Intangible Assets\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746911009697/R10.htm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "9\n",
      "AECOM Technology Corp\n",
      "2010-11-22\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746910009946/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-10-009946-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-10-009946.txt\n",
      "Directly parse .txt html file\n",
      "instance: b\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "instance: h1\n",
      "instance: strong\n",
      "instance: div\n",
      "10\n",
      "AECOM Technology Corp\n",
      "2009-11-27\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746909010452/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-09-010452-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-09-010452.txt\n",
      "Directly parse .txt html file\n",
      "instance: b\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "instance: h1\n",
      "instance: strong\n",
      "instance: div\n",
      "11\n",
      "AECOM Technology Corp\n",
      "2008-12-01\n",
      "----------------------------------------------------------------------------------------------------\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/000104746908012648/FilingSummary.xml\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-08-012648-index.htm\n",
      "https://www.sec.gov/Archives/edgar/data/0000868857/0001047469-08-012648.txt\n",
      "Directly parse .txt html file\n",
      "instance: b\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n",
      "finished get html\n",
      "finished get text\n",
      "finished refine html\n"
     ]
    }
   ],
   "source": [
    "relevant_table_links, failed_tables, failed_count = find_relevent_reports(firms_info, start_year=2007, \n",
    "                                                                          end_year=2017, max_word=70, \n",
    "                                                                          prev_year_tolerance=10, max_pages=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total runtime is ', str(end - start), ' seconds') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(firms_info['Ticker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unable to process', failed_count, '10-Ks in total.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_table_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_table_links.iloc[7, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file for RA\n",
    "relevant_table_links.to_csv('reports_links_for_NonTop_acq.csv')\n",
    "failed_tables.to_csv('failed_to_parse_for_NonTop_acq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_words = '''\n",
    "\n",
    "On October 17, 2014, the Company completed the acquisition of the U.S. headquartered URS Corporation (URS), an international provider of engineering, construction, and technical services, by purchasing 100% of the outstanding shares of URS common stock. The purpose of the acquisition was to further diversify the Company's market presence and accelerate the Company's strategy to create an integrated delivery platform for customers. The Company paid total consideration of approximately $2.3 billion in cash and issued approximately $1.6 billion of AECOM common stock to the former stockholders and certain equity award holders of URS. In connection with the acquisition, the Company also assumed URS's senior notes totaling $1.0 billion, and upon the occurrence of a change in control of URS, the URS senior noteholders had the right to redeem their notes at a cash price equal to 101% of the principal amount of the notes. Accordingly, on October 24, 2014, the Company purchased $0.6 billion of URS's senior notes from the noteholders. See also Note 9, Debt. Additionally, the Company repaid in full URS's $0.6 billion 2011 term loan and $0.1 billion of URS's revolving line of credit. Net assets acquired\n",
    " \n",
    "$\n",
    "\n",
    "5,120.6 Acquisitions occurring in fiscal 2011 and 2010 were recorded using the acquisition method of accounting in accordance with the revised guidance on business combinations, which the Company adopted on April 1, 2009. The results of acquired companies' operations have been included in the Company's consolidated financial statements since the effective date of each respective acquisition.\n",
    "Fiscal 2011 \n",
    "During fiscal 2011, the Company purchased eight businesses. The largest of these businesses was Conley Gas, Ltd. (\"Conley\"), a supplier of pure gases to the specialty gas industry with historical annual sales of approximately $9 million. A total of $21.2 million in cash was paid for the eight businesses and for the settlement of holdback liabilities and contingent consideration arrangements associated with acquisitions that were completed in years prior to fiscal 2010. Transaction and other integration costs incurred in fiscal 2011 and included in the Company's fiscal 2011 results were insignificant. These businesses had aggregate historical annual sales of approximately $21 million. Net sales from current year acquisitions that were included in the Company's fiscal 2011 results were $7 million. The Company acquired these businesses in order to expand its geographic coverage and strengthen its national network of branch-store locations. \n",
    "Total cash consideration The fair value of trade receivables acquired with fiscal 2011 acquisitions was $2.3 million, which approximated gross contractual amounts receivable. Goodwill associated with fiscal 2011 acquisitions was $7.3 million and is deductible for income tax purposes. Goodwill largely consists of expected synergies resulting from the acquisitions, including the expansion of geographical coverage that will facilitate the sale of industrial, medical and specialty gases and related supplies. Intangible assets related to fiscal 2011 acquisitions represent customer relationships and non-competition agreements and amounted to $5.6 million and $1.9 million, respectively. See Note 7 for further information on goodwill and intangible assets, including the allocation by segment. \n",
    "In connection with acquisitions prior to the April 1, 2009 adoption of the revised guidance on business combinations, the Company is a party to contingent payment agreements that provide for additional consideration to be paid to sellers if the future earnings of the acquired businesses exceed predetermined amounts. Amounts payable under contingent payment agreements continue through 2019 and are limited to $7.9 million. Such amounts, if paid, will be treated as additional costs of the acquisitions. \n",
    "Note 2:  Acquisitions and Collaborations\n",
    "LiRIS Acquisition\n",
    "On August 13, 2014, the Company completed the acquisition of LiRIS Biomedical, Inc. (LiRIS), a clinical-stage specialty pharmaceutical company based in the United States focused on developing a pipeline of innovative treatments for bladder diseases, for an upfront payment of $67.5 million, plus up to an aggregate of $295.0 million in payments contingent upon achieving certain future development milestones and up to an aggregate of $225.0 million in payments contingent upon achieving certain commercial milestones. The estimated fair value of the contingent consideration as of the acquisition date was $170.5 million. The acquisition was funded from current cash and equivalents balances.\n",
    "The Company recognized tangible and intangible assets acquired and liabilities assumed in connection with the LiRIS acquisition based on their estimated fair values at the acquisition date. The excess of the purchase price over the fair value of net assets acquired was recognized as goodwill. The goodwill acquired in the LiRIS acquisition is not deductible for federal income tax purposes. In connection with the acquisition, the Company acquired assets with a fair value of $307.8 million, consisting of intangible assets of $238.0 million and goodwill of $69.8 million, and assumed non-current deferred tax liabilities of $69.8 million. As of December 31, 2014, the total estimated fair value of the contingent consideration of $164.8 million was included in Other liabilities. \n",
    "The intangible assets consist of an in-process research and development asset of $225.3 million and a patented device technology asset of $12.7 million associated with LiRIS' proprietary delivery system that has an estimated useful life of 16 years. The in-process research and development asset relates to LiRIS lead investigational product for the treatment of interstitial cystitis and bladder pain syndrome which is currently in Phase II clinical trials. The estimated fair value of the in-process research and development asset was determined based on the use of a discounted cash flow model using an income approach. The in-process research and development asset is classified as an indefinite-lived intangible asset until the successful completion and commercialization or abandonment of the associated research and development efforts.\n",
    "Goodwill represents the excess of the LiRIS purchase price over the sum of the fair values assigned to assets acquired less liabilities assumed. The LiRIS acquisition has the potential to broaden the Company's product offering in urology in a disease state with high unmet needs, which the Company believes supports the amount of goodwill recognized as a result of the purchase price paid for LiRIS, in relation to other acquired tangible and intangible assets.\n",
    "The Company estimated the fair value of the contingent consideration liabilities related to the achievement of future development and regulatory milestones by assigning an achievement probability to each potential milestone and discounting the associated cash payment to its present value using a risk-adjusted rate of return. The Company estimated the fair value of the contingent consideration liabilities associated with sales milestones by employing Monte Carlo simulations to estimate the volatility and systematic relative risk of acquired product revenues and discounting the associated cash payment amounts to their present values using a credit-risk-adjusted interest rate.\n",
    "MAP Acquisition\n",
    "On March 1, 2013, the Company completed the acquisition of MAP Pharmaceuticals, Inc. (MAP), a biopharmaceutical company based in the United States focused on developing and commercializing new therapies in neurology, including SempranaTM, formerly referred to as Levadex, an orally inhaled drug for the potential acute treatment of migraine in adults, for an aggregate purchase price of approximately $871.7 million, net of cash acquired. The acquisition was funded from a combination of current cash and equivalents and short-term investments.\n",
    "The Company recognized tangible and intangible assets acquired and liabilities assumed in connection with the MAP acquisition based on their estimated fair values at the acquisition date. The excess of the purchase price over the fair value of net assets acquired was recognized as goodwill. The goodwill acquired in the MAP acquisition is not deductible for federal income tax purposes. In connection with the acquisition, the Company acquired assets with a fair value of $1,233.6 million, consisting of current assets of $2.3 million, property, plant and equipment of $7.7 million, other non-current assets of $0.3 million, deferred tax assets of $132.7 million, intangible assets of $915.6 million and goodwill of $175.0 million, and assumed liabilities of $361.9 million, consisting of current liabilities of $27.3 million and deferred tax liabilities of $334.6 million.\n",
    "The intangible assets consist of an in-process research and development asset of $683.5 million associated with SempranaTM and a patented device technology asset of $232.1 million associated with MAP's proprietary Tempo delivery system that has an estimated useful life of 15 years.\n",
    "Goodwill represents the excess of the MAP purchase price over the sum of the amounts assigned to assets acquired less liabilities assumed. The MAP acquisition broadens the Company's product offering for the treatment of migraine headaches and MAP's proprietary drug particle and inhalation technology provides the potential for new product development opportunities, which the Company believes support the amount of goodwill recognized as a result of the purchase price paid for MAP, in relation to other acquired tangible and intangible assets.\n",
    "Exemplar Acquisition\n",
    "On April 12, 2013, the Company completed the acquisition of Exemplar Pharma, LLC (Exemplar), a third party contract manufacturer for MAP's Tempo delivery system, for an aggregate purchase price of approximately $16.1 million, net of cash acquired. Prior to the acquisition, the Company also had a $1.9 million payable to Exemplar, which was effectively settled upon the acquisition. In connection with the acquisition, the Company acquired assets with a fair value of $16.6 million, consisting of current assets of $0.5 million, property, plant and equipment of $2.1 million and goodwill of $14.0 million, and assumed current liabilities of $0.5 million. The goodwill acquired in the Exemplar acquisition is deductible for federal income tax purposes.\n",
    "Medytox Collaboration\n",
    "On September 25, 2013, the Company announced that it had entered into a license agreement with Medytox, Inc. (Medytox), contingent on obtaining certain government approvals. In January 2014, the Company closed the transaction. Under the terms of the agreement, the Company made an upfront payment to Medytox of $65.0 million in January 2014 and Medytox granted the Company exclusive rights, worldwide outside of Korea with co-exclusive rights in Japan, to develop and, if approved, commercialize certain neurotoxin product candidates currently in development, including a potential liquid-injectable product. The upfront payment of $65.0 million was recorded as research and development (R&D) expense in the first quarter of 2014 because the technology has not yet achieved regulatory approval. The terms of the agreement also include potential future development milestone payments of up to $116.5 million and potential future sales milestone payments of up to $180.5 million, as well as potential future royalty payments. In the third quarter of 2014, the Company made a development milestone payment to Medytox of $15.0 million, which was recorded as R&D expense because the technology has not yet achieved regulatory approval.\n",
    "Other Acquisitions and Collaborations\n",
    "In March 2014, the Company completed the acquisition of certain assets from Aline Aesthetics, LLC and Tautona Group, L.P. for an upfront payment of $10.0 million and potential future payments for certain milestone events. The Company accounted for the acquisition as a purchase of net assets. The acquired assets primarily consist of intellectual property related to technology under development for use as a dermal filler that has not achieved regulatory approval. The upfront payment was accrued and recorded as R&D expense in the first quarter of 2014 and was paid in the second quarter of 2014.\n",
    "In November 2013, the Company purchased a noncontrolling interest in a subsidiary from a minority shareholder for $18.0 million. The Company accounted for the purchase as an equity transaction and recorded the difference between the cash consideration and the carrying amount of the noncontrolling interest, including its share of accumulated other comprehensive income, as a decrease in additional paid-in capital of $1.3 million. \n",
    "On September 10, 2013, the Company entered into a license and collaboration agreement with a third party pursuant to which the Company obtained exclusive global rights to research, manufacture and commercialize certain technologies for the treatment of ocular disease. Under the terms of the agreement, the Company made a $6.5 million upfront payment, which was recorded as R&D expense in the third quarter of 2013 because the technology has not yet achieved regulatory approval. The terms of the agreement also include potential future payments to the third party related to the Companys achievement of development, regulatory and sales milestone events, as well as potential future royalty payments.\n",
    "In connection with various business development transactions where the Company has outlicensed its technology to third parties, the Company has aggregate potential future milestone receipts of approximately $45.9 million as of December 31, 2014, none of which are individually significant. Of that amount, approximately $3.5 million relates to achievement of certain development milestones, approximately $17.0 million relates to achievement of certain regulatory milestones, and approximately $25.4 million relates to achievement of certain commercial sales milestones. Due to the challenges associated with developing and obtaining approval for pharmaceutical products, there is substantial uncertainty whether any of the future milestones will be achieved. The Company evaluates whether milestone payments are substantive based on the facts and circumstances associated with each milestone payment.\n",
    "The Company believes that the fair values assigned to the assets acquired and liabilities assumed for its acquisitions were based on reasonable assumptions. The Company's fair value estimates may change during the allowable measurement period, which is up to one year from the acquisition date, if additional information becomes available.\n",
    "NOTE 4  Acquisitions and Other Agreements\n",
    "Acquisition of Warner Chilcott\n",
    "On October 1, 2013, the Company completed the Warner Chilcott Acquisition in a stock for stock transaction for a value, including the assumption of debt, of $9.2 billion. Warner Chilcott was a leading specialty pharmaceutical company focused on the womens healthcare, gastroenterology, urology and dermatology segments of the branded pharmaceuticals market, primarily in North America.\n",
    "Recognition and Measurement of Assets Acquired and Liabilities Assumed at Fair Value\n",
    "The transaction has been accounted for using the acquisition method of accounting. This method requires that assets acquired and liabilities assumed in a business combination be recognized at their fair values as of the acquisition date. As of December 31, 2013, certain amounts relating to SRA reserves have not been finalized. The finalization of these matters may result in changes to goodwill and the Company expects to finalize such matters in 2014.\n",
    "Consideration\n",
    "The total consideration for the Warner Chilcott Acquisition of $5,833.9 million is comprised of the equity value of shares that were outstanding and vested prior to October 1, 2013 ($5,761.3 million) and the portion of outstanding equity awards deemed to have been earned as of October 1, 2013 ($72.6 million). The portion deemed not to have been earned ($77.4 million) as of October 1, 2013 will be expensed over the remaining future vesting period, including $45.4 million relating to Warner Chilcott restructuring charges recognized in the year ended December 31, 2013.\n",
    "Inventories\n",
    "The fair value of inventories acquired included a step-up in the value of inventories of $408.3 million. In the year ended December 31, 2013, the Company recognized $173.5 million as a component of cost of sales as the inventory acquired on October 1, 2013 was sold to the Companys customers.\n",
    "IPR&D and Intangible Assets\n",
    "IPR&D intangible assets represent the value assigned to acquired R&D projects that, as of the acquisition date, had not established technological feasibility and had no alternative future use. The IPR&D intangible assets are capitalized and accounted for as indefinite-lived intangible assets and will be subject to impairment testing until completion or abandonment of the projects. Upon successful completion of each project and launch of the product, the Company will make a separate determination of the estimated useful life of the IPR&D intangible asset and the related amortization will be recorded as an expense over the estimated useful life (IPR&D Acquisition Accounting). Intangible assets represent CMPs and IPR&D and have an estimated weighted average useful life of 2.7 years.\n",
    "The estimated fair value of the IPR&D and identifiable intangible assets was determined using the income approach, which is a valuation technique that provides an estimate of the fair value of an asset based on market participant expectations of the cash flows an asset would generate over its remaining useful life. Some of the more significant assumptions inherent in the development of those asset valuations include the estimated net cash flows for each year for each asset or product (including net revenues, cost of sales, R&D costs, selling and marketing costs and working capital/asset contributory asset charges), the appropriate discount rate to select in order to measure the risk inherent in each future cash flow stream, the assessment of each assets life cycle, the potential regulatory and commercial success risks, competitive trends impacting the asset and each cash flow stream as well as other factors (the IPR&D and Intangible Asset Valuation Technique). The discount rates used to arrive at the present value at the acquisition date of CMPs was 8.0% and for IPR&D ranged from 8.0% to 9.0%, to reflect the internal rate of return and incremental commercial uncertainty in the cash flow projections. No assurances can be given that the underlying assumptions used to prepare the discounted cash flow analysis will not change. For these and other reasons, actual results may vary significantly from estimated results.\n",
    " \n",
    "The following table identifies the summarized amounts recognized and the weighted average useful lives of intangible assets:\n",
    "\n",
    "Acquisition of Warner Chilcott\n",
    "On October 1, 2013, the Company completed the Warner Chilcott Acquisition in a stock for stock transaction for a value, including the assumption of debt, of $9.2 billion. Warner Chilcott was a leading specialty pharmaceutical company focused on the womens healthcare, gastroenterology, urology and dermatology segments of the branded pharmaceuticals market, primarily in North America.\n",
    "Recognition and Measurement of Assets Acquired and Liabilities Assumed at Fair Value\n",
    "The transaction has been accounted for using the acquisition method of accounting. This method requires that assets acquired and liabilities assumed in a business combination be recognized at their fair values as of the acquisition date. As of December 31, 2013, certain amounts relating to SRA reserves have not been finalized. The finalization of these matters may result in changes to goodwill and the Company expects to finalize such matters in 2014\n",
    "\n",
    "Acquisition of Medicines360\n",
    "On June 11, 2013, the Company entered into an exclusive license agreement with Medicines360 to market, sell and distribute Medicines360s LNG20 intrauterine device (LNG 20) in the U.S. and in Canada for a payment of approximately $52.3 million. According to the terms of the agreement, the Company is also required to pay Medicines360 certain regulatory and sales based milestone payments totaling up to $125.0 million plus royalties (the Medicines360 Acquisition). Medicines360 retained the rights to market the product in the U.S. public sector, including family planning clinics that provide services to low-income women. LNG20, originally developed by Uteron Pharma S.P.R.L. in Belgium (now a subsidiary of the Company), is designed to deliver 20 mcg of levonorgestrel per day for the indication of long-term contraception, and is currently in Phase III clinical trials in the United States. Pending FDA approval, the LNG20 product could be launched in the U.S. as early as 2014. The transaction has been accounted for using the acquisition method of accounting. This method requires that assets acquired and liabilities assumed in a business combination be recognized at their respective fair values as of the acquisition date. In connection with the acquisition, the Company recorded $191.7 million in IPR&D, $6.7 million in prepaid R&D and contingent consideration of $146.1 million\n",
    "\n",
    "Unaudited Pro Forma Results of Operations\n",
    "Pro forma results of operations have not been presented because the effect of the Medicines360 Acquisition was not material.\n",
    " Acquisition of Actavis Group\n",
    "On October 31, 2012, the Company completed the Actavis Group Acquisition. The Actavis Group was a privately held generic pharmaceutical company specializing in the development, manufacture and sale of generic pharmaceuticals. With the Actavis Group Acquisition, the Company significantly expanded its international market presence in established markets including Europe and MEAAP. In addition, the acquisition expanded the Companys product portfolio and pipeline in modified release, solid oral dosage and transdermal products into semi-solids, liquids and injectables. Actavis Group results are included in the Actavis Pharma and Actavis Specialty Brands segments as of the acquisition date.\n",
    "The Company funded the cash portion of the transaction through a combination of term loan borrowings and senior unsecured notes. For additional information, refer to Note 13  Long-term Debt.\n",
    "Recognition and Measurement of Assets Acquired and Liabilities Assumed at Fair Value\n",
    "The transaction has been accounted for using the acquisition method of accounting. The following table summarizes the final fair values of the tangible and identifiable intangible assets acquired and liabilities assumed at the acquisition date:\n",
    "Acquisition of Ascent Pharmahealth Ltd.\n",
    "On January 24, 2012, the Company acquired all of the outstanding equity of Ascent Pharmahealth Ltd. (Ascent) the Australian and Southeast Asian generic pharmaceutical business of Strides Arcolab Ltd. for AU$376.6 million, or approximately $392.6 million, including working capital adjustments (the Ascent Acquisition). As a result of the acquisition, the Company enhanced its commercial presence in Australia and gained selling and marketing capabilities in Southeast Asia. In Australia, Ascent markets generic, brands, OTC and dermatology and skin care products. In Southeast Asia, Ascent markets generic and OTC products. Ascents Southeast Asian business includes commercial operations in Singapore, Malaysia, Hong Kong, Vietnam and Thailand. Ascent operates a manufacturing facility in Singapore for generic products in Southeast Asian markets. Ascents results are included in the Actavis Pharma segment as of the acquisition date.\n",
    "Pro forma results of operations have not been presented because the effect of the acquisition was not material.\n",
    "Acquisition of Specifar\n",
    "On May 25, 2011, the Company and each of the shareholders (together, the Sellers) of Paomar PLC (Paomar) entered into a stock purchase agreement pursuant to which the Company purchased all of the outstanding equity of Paomar for cash totaling 400.0 million, or approximately $561.7 million at closing, subject to a net of working capital adjustment of 1.5 million, or approximately $2.2 million, and certain contingent consideration (the Specifar Acquisition). Paomar is a company incorporated under the laws of Cyprus and owner of 100 percent of the shares of Specifar Commercial Industrial Pharmaceutical, Chemical and Construction Exploitations Societe Anonyme, a company organized under the laws of Greece. Specifar owns 100 percent of the shares of Alet Pharmaceuticals Industrial and Commercial Societe Anonyme (Alet). The contingent consideration due to the Specifar Acquisition (not to exceed an aggregate total of 40.0 million) is based on the gross profits on sales of the generic tablet version of Nexium (esomeprazole) developed by Specifar during its first five years of sales in countries including major markets in Europe, Asia and Latin America, as well as in Canada. For additional information on the contingent payment, refer to NOTE 20  Fair Value Measurements.\n",
    "Through the Specifar Acquisition, the Company gained a generic pharmaceuticals product development company that develops and out-licenses generic pharmaceutical products primarily in Europe. In addition, the acquisition enhanced the Companys commercial presence in key European markets by providing a portfolio of products and provides a commercial presence in the branded-generic Greek pharmaceuticals market, including the Specifar and Alet brands of products. The Company funded the transaction using cash on hand and borrowings from the Companys credit facility. Specifar results are included in the Actavis Pharma segment subsequent to the acquisition date.\n",
    "\n",
    "Property Acquisitions \n",
    "\n",
    "Properties Under Development\n",
    "During the year ended December 31, 2016, the Company secured the following in-process development properties for approximately $39.6 million. Total cash consideration of $40.7 million consisted of escrow deposits and cash paid at closing:\n",
    "\n",
    "Since their respective acquisition dates, the operating properties discussed above contributed a combined $1.9 million of revenues for the year ended December 31, 2016.  These properties had a combined net loss of $0.1 million for the year ended December 31, 2016, which includes $0.3 million of acquisition-related costs such as broker fees, due diligence costs and legal and accounting fees that are included in wholly-owned properties operating expense on the accompanying consolidated statements of comprehensive income. \n",
    "\n",
    "During 2015, the Company acquired eight wholly-owned properties containing 4,061 beds for a combined purchase price of approximately $378.3 million. As part of these transactions, the Company assumed approximately $69.4 million of mortgage debt and issued 343,895 Common OP Units, valued at $41.24 per unit. \n",
    "\n",
    "During 2014, the Company acquired one property comprised of 610 beds located near the University of Georgia and a property containing a hotel near the University of Colorado campus. The existing hotel at the University of Colorado was demolished in the fourth quarter 2014 and a new 398-bed student housing facility was constructed and opened for occupancy in August 2016. The total consideration for these two acquisitions was approximately $75.1 million.\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "words_list = re.sub(r'[^\\w\\s]','',note_words).replace('\\n', '').lower().split()\n",
    "words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(words_list, return_counts=True)\n",
    "sort_index = np.argsort(counts_elements)[::-1]\n",
    "unique_elements = unique_elements[sort_index]\n",
    "counts_elements = counts_elements[sort_index]\n",
    "unique_elements[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#convert list to string and generate\n",
    "unique_string=(' ').join(words_list)\n",
    "wordcloud = WordCloud(width = 1000, height = 500).generate(unique_string)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "#plt.savefig(\"wordcloud_in_acq_notes\"+\".png\", bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#i guess most frequent are:\n",
    "#['acquisition', 'acquire', 'merger', 'business combination', 'asset', 'million', 'billion',\n",
    "#'fair', 'value', 'purchase price', 'price', 'paid', 'goodwill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#circle instead of rectangle\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", repeat=True, mask=mask).generate(' '.join(words_list))\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(\"wordcloud_in_acq_notes_circle\"+\".png\", bbox_inches='tight')\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is for changing the fileppath for files I generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_fp = os.path.realpath(os.getcwd())\n",
    "\n",
    "reports_fp = relative_fp + '\\\\reports_links_for_NonTop_acq.csv'\n",
    "reports_df = pd.read_csv(reports_fp)\n",
    "reports_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "for i in range(len(reports_df)):\n",
    "    link_col_index = 4\n",
    "    if reports_df.iloc[i, 4].endswith('.html'):\n",
    "        reports_df.iloc[i, 4] = r'http://makeresearchgreatagain.com/sec_files/' + reports_df.iloc[i, 4]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "reports_df.to_csv('reports_links_for_NonTop_acq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
